{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling\n",
        "## `! git clone https://github.com/DS3001/wrangling`\n",
        "## Do Q2, and one of Q1 or Q3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  - This paper is about data tidying- when you have a tidy dataset, it is easy to manipulate, model, and visualize. They describe a tidy dataset as each observation as a row and each variable as a column, and each type of observational unit is a table. This structure makes it both easy to tidy up messy datasets and makes it easy to develop tidy tools for analysis. \n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  - The purpose of the tidy data standard is to provide a standard way to organize data values within a dataset. It makes data cleaning easier, because you don't need to \"reinvent the wheel\", or figure out new ways to clean the data each time. Following a standard will make your life easier for each dataset you want to clean. \n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  - The first sentence I interpreted as meaning tidy datasets are all alike, making it easier to clean and perform operations on. Messy datasets though have their own unique problems and difficulties if you are going to try and clean them, and each one of those problems may be different among each messy dataset. \n",
        "  - The second sentence I interpreted as meaning that different people may define observations and variables differently. If you made the dataset, it would be easy for you to define your observations and variables, but for people looking at the dataset, they may have trouble precisely defining the variables and observations, or they may get observations or variables mixed up or not understand their meaning. This miscommunication emphasizes the need for good documentation of your datasets, so you can explain the observations, the variables, their meanings and their purpose, so that anyone attempting to clean the dataset won't have a tough time.\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  - Wickham says that a dataset is a collection of values, usually either numbers or strings. Values are organized in two ways- every value belongs to a variable and an observation. Also, he says that a variable contains all values that measure the same underlying attribute- for example- height, temperature, duration- across units. He also says that an observation contains all values measured on the same unit- like a person, day, or race.\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  - In section 2.3, tidy data is defined as the standard way of mapping the meaning of a dataset to its structure. The 3 main points to this are:\n",
        "    - Each variable forms a column\n",
        "    -  Each observation forms a row\n",
        "    -  Each type of observational unit forms a table\n",
        "  - If the data doesn't follow these standards, it isn't tidy, meaning it's messy.\n",
        "\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  - The 5 most common problems are \n",
        "    - Column headers are values, not variable names.\n",
        "    - Multiple variables are stored in one column\n",
        "    - Variables are stored in both rows and columns\n",
        "    - Multiple types of observational units are stored in the same table\n",
        "    - A single observational unit is stored in multiple tables\n",
        "  - The data in table 4 are messy because the columns are really just values of income, which should be its own column as a variable. \n",
        "  - Melting a dataset refers to turning columns into rows- like what we are doing with income here.\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  - Table 11 is messy due to a similar problem as the last question- it has days along the top as the columns- which should be its own column as days are values. Table 12a is molten because it melted the variable days into date, but it is not tidy yet because instead of values, the element column contains names of variables. Table 12b is tidy because tmin and tmax got split into two variables, and each row represents the meteorological measurements for a specific day. \n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?\n",
        "  - The chicken-and-egg problem refers to how tidy is connected to the tools used for cleaning- if tidy data is only as useful as the tools that work with it, then they will be linked. This makes it easy to get stuck in a state where changing data structures or data tools won't improve your workflow. Wickham hopes that in the future, with further work on the subject of data wrangling, that the concept of tidy and cleaning data won't just train people to use these tidy tools, but to further data science and allow for more innovative tools and ideas to be created."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fa033b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "- Q2.1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4ef47703",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/48/qjmd8jwx7l36c_c6n_c769_h0000gn/T/ipykernel_93220/810165657.py:1: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array(['145', '37', '28', '199', '549', '149', '250', '90', '270', '290',\n",
              "       '170', '59', '49', '68', '285', '75', '100', '150', '700', '125',\n",
              "       '175', '40', '89', '95', '99', '499', '120', '79', '110', '180',\n",
              "       '143', '230', '350', '135', '85', '60', '70', '55', '44', '200',\n",
              "       '165', '115', '74', '84', '129', '50', '185', '80', '190', '140',\n",
              "       '45', '65', '225', '600', '109', '1,990', '73', '240', '72', '105',\n",
              "       '155', '160', '42', '132', '117', '295', '280', '159', '107', '69',\n",
              "       '239', '220', '399', '130', '375', '585', '275', '139', '260',\n",
              "       '35', '133', '300', '289', '179', '98', '195', '29', '27', '39',\n",
              "       '249', '192', '142', '169', '1,000', '131', '138', '113', '122',\n",
              "       '329', '101', '475', '238', '272', '308', '126', '235', '315',\n",
              "       '248', '128', '56', '207', '450', '215', '210', '385', '445',\n",
              "       '136', '247', '118', '77', '76', '92', '198', '205', '299', '222',\n",
              "       '245', '104', '153', '349', '114', '320', '292', '226', '420',\n",
              "       '500', '325', '307', '78', '265', '108', '123', '189', '32', '58',\n",
              "       '86', '219', '800', '335', '63', '229', '425', '67', '87', '1,200',\n",
              "       '158', '650', '234', '310', '695', '400', '166', '119', '62',\n",
              "       '168', '340', '479', '43', '395', '144', '52', '47', '529', '187',\n",
              "       '209', '233', '82', '269', '163', '172', '305', '156', '550',\n",
              "       '435', '137', '124', '48', '279', '330', '5,000', '134', '378',\n",
              "       '97', '277', '64', '193', '147', '186', '264', '30', '3,000',\n",
              "       '112', '94', '379', '57', '415', '236', '410', '214', '88', '66',\n",
              "       '71', '171', '157', '545', '1,500', '83', '96', '1,800', '81',\n",
              "       '188', '380', '255', '505', '54', '33', '174', '93', '740', '640',\n",
              "       '1,300', '440', '599', '357', '1,239', '495', '127', '5,999',\n",
              "       '178', '348', '152', '242', '183', '253', '750', '259', '365',\n",
              "       '273', '197', '397', '103', '389', '355', '559', '38', '203',\n",
              "       '999', '141', '162', '333', '698', '46', '360', '895', '10', '41',\n",
              "       '206', '281', '449', '388', '212', '102', '201', '2,750', '4,750',\n",
              "       '432', '675', '167', '390', '298', '339', '194', '302', '211',\n",
              "       '595', '191', '53', '361', '480', '8,000', '4,500', '459', '997',\n",
              "       '345', '216', '218', '111', '735', '276', '91', '490', '850',\n",
              "       '398', '36', '775', '267', '625', '336', '2,500', '176', '725',\n",
              "       '3,750', '469', '106', '460', '287', '575', '227', '263', '25',\n",
              "       '228', '208', '177', '880', '148', '116', '685', '470', '217',\n",
              "       '164', '61', '645', '699', '405', '252', '319', '268', '419',\n",
              "       '343', '525', '311', '840', '154', '294', '950', '409', '184',\n",
              "       '257', '204', '241', '2,000', '412', '121', '288', '196', '900',\n",
              "       '647', '524', '1,750', '309', '510', '1,495', '1,700', '799',\n",
              "       '383', '372', '492', '327', '1,999', '656', '224', '173', '875',\n",
              "       '1,170', '795', '690', '146', '465', '1,100', '151', '274', '429',\n",
              "       '825', '282', '256', '1,111', '620', '271', '161', '51', '855',\n",
              "       '579', '1,174', '430', '20', '899', '649', '485', '181', '455',\n",
              "       '4,000', '243', '342', '590', '560', '374', '437', '232', '359',\n",
              "       '985', '31', '244', '254', '723', '237', '428', '370', '34',\n",
              "       '1,400', '580', '2,520', '221', '749', '1,600', '2,695', '306',\n",
              "       '202', '680', '570', '520', '223', '2,295', '213', '1,065', '346',\n",
              "       '24', '286', '296', '266', '26', '995', '1,368', '393', '182',\n",
              "       '635', '258', '780', '589', '347', '1,250', '1,350', '446',\n",
              "       '3,200', '1,050', '1,650', '1,550', '975', '323', '6,500', '2,499',\n",
              "       '1,850', '2,250', '715', '461', '540', '356', '439', '384', '569',\n",
              "       '1,900', '22', '785', '626', '830', '318', '444', '321', '401',\n",
              "       '1,499', '888', '369', '770', '386', '366', '344', '630', '313',\n",
              "       '597', '262', '509', '10,000', '278', '312', '789', '1,195', '422',\n",
              "       '21', '765', '3,500', '945', '326', '3,100', '2,486', '3,390',\n",
              "       '1,356', '2,599', '472', '454', '328', '396', '291'], dtype=object)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "df = pd.read_csv(\"./data/airbnb_hw.csv\", low_memory=False)\n",
        "\n",
        "price = df['Price']\n",
        "price.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9317d042",
      "metadata": {},
      "source": [
        "We can see the problem with the price variable is that it was imported as a string, not a number, because we can still see the comma separator. In order to fix this, we have to change it into a number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ea2ef72a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 missing values.\n"
          ]
        }
      ],
      "source": [
        "# What we can do is convert the price variable using pd.to_numeric, but we have to be careful with the commas.\n",
        "# So, we can just keep it as a string at first, then replace all the commas with nothing.\n",
        "# Then, once we convert to a numeric number, we won't lose all the values greater than 999.\n",
        "price_variable = df['Price']\n",
        "price_variable = price_variable.str.replace(',','') \n",
        "price_variable = pd.to_numeric(price_variable,errors='coerce') \n",
        "\n",
        "print(sum(price_variable.isnull()), \"missing values.\") # show all the missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f75554d",
      "metadata": {},
      "source": [
        "Using this method, we end up with 0 missing values. Converting a variable which is a string representation of a number is easy, we just have to make sure to account for the commas if they are there."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c99601e7",
      "metadata": {},
      "source": [
        "- Q2.2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "31bcaec5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Type\n",
              "Unprovoked             4716\n",
              "Provoked                593\n",
              "Invalid                 552\n",
              "Sea Disaster            239\n",
              "Watercraft              142\n",
              "Boat                    109\n",
              "Boating                  92\n",
              "Questionable             10\n",
              "Unconfirmed               1\n",
              "Unverified                1\n",
              "Under investigation       1\n",
              "Boatomg                   1\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"./data/sharks.csv\", low_memory=False)\n",
        "df.head()\n",
        "df.columns.tolist()\n",
        "df['Type'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7043cfbd",
      "metadata": {},
      "source": [
        "As we can see, there are a lot of values relating to water, or boating. If we replace all those different types and put them in one category, it will be a lot easier to read this dataset for the type variable.  Also, I am assuming that \"boatomg\" should be under boating as well. Similarly, there are categories that don't make sense- the invalid/under investigation/unverified cases. We should just remove these completely as they are unclean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "db130997",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type\n",
            "Unprovoked    4716\n",
            "Provoked       593\n",
            "Watercraft     583\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "type_temp = df['Type'] \n",
        "\n",
        "# Replace all boat/water values into one category - Watercraft. \n",
        "type_temp = type_temp.replace(['Boatomg', 'Boat', 'Sea Disaster', 'Boating'],'Watercraft') \n",
        "\n",
        "type_temp = type_temp.replace(['Questionable', 'Unconfirmed','Unverified','Invalid','Under investigation'],np.nan) # All unclean values\n",
        "print(type_temp.value_counts())\n",
        "# We can see the new value counts and how it is a lot more clean\n",
        "# Now, we want to remove the temporary df vector we made for type, but before we do that, we replace the df's Type variable with our newly created vector.\n",
        "df['Type'] = type_temp\n",
        "del type_temp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b310acc",
      "metadata": {},
      "source": [
        "- Q2.3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "75b5cfd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since the dataset is large, lets get it from the web\n",
        "url = 'http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv'\n",
        "df = pd.read_csv(url,low_memory=False) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "86e20062",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9 0 1] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "defendantWasReleased = df['WhetherDefendantWasReleasedPretrial']\n",
        "print(defendantWasReleased.unique(),'\\n')\n",
        "\n",
        "# From unique, we see there are 3 values - 1, 0, and 9. From the codebook, 9 means unclear. So, we can replace 9 with np.nan\n",
        "defendantWasReleased = defendantWasReleased.replace(9,np.nan) \n",
        "# Now we replace the actual vector in the dataframe with our temp vector, and delete the temp vector\n",
        "df['WhetherDefendantWasReleasedPretrial'] = defendantWasReleased \n",
        "del defendantWasReleased"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7095da7",
      "metadata": {},
      "source": [
        "- Q2.4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bc67437e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' ' '60' '12' '.985626283367556' '36' '6' '24' '5.91375770020534' '120'\n",
            " '72' '11.9917864476386' '0' '2.95687885010267' '84' '108' '300' '240'\n",
            " '180' '4' '96' '2' '54' '.328542094455852' '44' '5' '115' '132' '48'\n",
            " '258' '34' '76' '.164271047227926' '.131416837782341' '111' '9' '3'\n",
            " '1.97125256673511' '36.9856262833676' '.0657084188911704'\n",
            " '35.4928131416838' '106.492813141684' '8' '35' '18.3141683778234' '480'\n",
            " '32' '93' '234' '732' '1.16427104722793' '4.6570841889117' '21' '7'\n",
            " '4.49281314168378' '18' '600' '43.1642710472279' '179' '52' '30' '20'\n",
            " '192' '702' '14' '55' '53' '11.9055441478439' '114' '35.0061601642711'\n",
            " '68' '.657084188911704' '46.6242299794661' '102' '65' '200' '57'\n",
            " '24.3285420944559' '12.1642710472279' '117' '81.4928131416838'\n",
            " '22.4928131416838' '1980' '3.6570841889117' '56' '10' '2.79260780287474'\n",
            " '1' '47' '22' '1500' '40' '284' '11' '118' '42' '162' '156'\n",
            " '47.2956878850103' '105' '51' '246' '29' '75' '324' '360'\n",
            " '34.4804928131417' '120.328542094456' '59.9260780287474' '66'\n",
            " '59.9917864476386' '660' '51.1642710472279' '14.9568788501027'\n",
            " '3.98562628336756' '78' '228' '1.47843942505133' '62' '4.8' '86' '168'\n",
            " '23' '33' '48.0328542094456' '720' '348' '1200' '27' '49' '87' '420' '63'\n",
            " '79.9260780287474' '57.0349075975359' '49.9712525667351'\n",
            " '59.4928131416838' '17' '238.492813141684' '60.9856262833676' '126' '45'\n",
            " '158' '216' '227' '42.9568788501027' '445' '70.952772073922' '516'\n",
            " '177.82135523614' '1752' '90' '1080' '141' '4.82956878850103' '230' '31'\n",
            " '2208' '52.5133470225873' '69' '26' '33.4928131416838' '140' '131' '344'\n",
            " '219' '101' '71' '59' '58' '120.197125256674' '67' '35.4004106776181'\n",
            " '3.28542094455852' '40.1642710472279' '91' '1.7741273100616' '155'\n",
            " '34.4928131416838' '81' '92.3285420944559' '3.5482546201232' '207' '74'\n",
            " '518' '28' '8.95687885010267' '237' '404.673511293634' '18.1642710472279'\n",
            " '10.7433264887064' '551' '39' '15' '124' '43' '176' '19.4928131416838'\n",
            " '482' '129' '88' '46' '45.8542094455852' '128.628336755647'\n",
            " '136.492813141684' '108.328542094456' '50' '363.663244353183' '288' '250'\n",
            " '107' '81.0225872689938' '444' '205' '10.6570841889117' '19'\n",
            " '66.9856262833676' '38.4928131416838' '264' '276' '173' '222' '144' '294'\n",
            " '336' '431' '450' '73' '99.3285420944559' '128' '30.8069815195072'\n",
            " '31.5256673511294' '127' '202' '55.3285420944559' '89' '242'\n",
            " '1.31416837782341' '1029' '.788501026694045' '194.858316221766' '399'\n",
            " '39.6570841889117' '56.95687885' '198' '120.985626283368'\n",
            " '47.6570841889117' '148' '6.8993839835729' '65.3285420944559'\n",
            " '5.95277207392197' '.0985626283367557' '3.32854209445585'\n",
            " '3.94250513347023' '12.9856262833676' '6.98562628336756'\n",
            " '13.1498973305955' '15.1642710472279' '17.1971252566735'\n",
            " '17.9137577002053' '104' '212' '24.6570841889117' '72.6570841889117'\n",
            " '2.98562628336756' '144.985626283368' '31.9712525667351' '183'\n",
            " '4.98562628336756' '11.8213552361396' '252' '12.394250513347'\n",
            " '42.4928131416838' '10.1642710472279' '11.1642710472279'\n",
            " '5.49281314168378' '59.6632443531827' '12.3285420944559'\n",
            " '48.9856262833676' '240.985626283368' '2.6570841889117' '540'\n",
            " '2.97125256673511' '6.32854209445585' '23.6632443531828'\n",
            " '133.657084188912' '35.3285420944559' '456' '103' '1.72279260780287'\n",
            " '12.6570841889117' '11.6570841889117' '60.3285420944559'\n",
            " '3.78850102669405' '576' '2.13141683778234' '492' '14.9856262833676'\n",
            " '24.9856262833676' '61.9712525667351' '5.6570841889117' '16'\n",
            " '42.1642710472279' '.492813141683778' '138' '13.3141683778234'\n",
            " '11.8932238193018' '5.32854209445585' '95' '62.6570841889117'\n",
            " '3.08829568788501' '11.8275154004107' '1.64271047227926'\n",
            " '47.9917864476386' '4.27104722792608' '8.32854209445585'\n",
            " '3.31416837782341' '70' '77' '1.09856262833676' '48.1642710472279'\n",
            " '27.4928131416838' '6.93839835728953' '1011' '.68993839835729'\n",
            " '1.1170431211499' '1.49281314168378' '4.16427104722793'\n",
            " '1.19712525667351' '4.07392197125257' '188' '11.3285420944559'\n",
            " '.0328542094455852' '432' '11.952772073922' '36.4928131416838'\n",
            " '23.9835728952772' '9.98562628336756' '98' '36.3285420944559' '112'\n",
            " '.394250513347023' '13' '.262833675564682' '13.7987679671458'\n",
            " '5.8870636550308' '354' '5.91991786447639' '24.1642710472279'\n",
            " '62.95687885' '4.59958932238193' '123' '2.32854209445585'\n",
            " '23.9240246406571' '204' '197' '174' '16.1498973305955' '840' '440'\n",
            " '98.95687885' '17.952772073922' '63.9425051334702' '60.1314168377823'\n",
            " '12.1314168377823' '172.952772073922' '.197125256673511'\n",
            " '138.164271047228' '4.92813141683778' '.919917864476386'\n",
            " '18.9856262833676' '6.6570841889117' '2.85420944558522'\n",
            " '8.91375770020534' '146' '12.4928131416838' '.558521560574949'\n",
            " '.722792607802875' '5.82135523613963' '84.9856262833676'\n",
            " '6.16427104722793' '15.9856262833676' '64.5585215605749'\n",
            " '38.299794661191' '11.958932238193' '3.1211498973306' '126.328542094456'\n",
            " '5.16427104722793' '64' '42.6570841889117' '312' '19.9712525667351'\n",
            " '82.3285420944559' '23.9712525667351' '17.6242299794661'\n",
            " '121.971252566735' '59.6550308008214' '1.32854209445585'\n",
            " '7.97125256673511' '1.91991786447639' '.525667351129363'\n",
            " '9.32854209445585' '42.9856262833676' '41.9137577002053'\n",
            " '72.9856262833676' '12.4784394250513' '5.19096509240246' '473'\n",
            " '16.6570841889117' '109' '86.3285420944559' '41' '1.90554414784394'\n",
            " '94.1642710472279' '302' '4.39425051334702' '10.8213552361396'\n",
            " '18.3285420944559' '154' '83' '110.956878850103' '226' '96.0328542094456'\n",
            " '4.82135523613963' '30.3285420944559' '37.9712525667351'\n",
            " '50.4640657084189' '286' '99' '99.4928131416838' '2.6611909650924'\n",
            " '70.9712525667351' '13.9712525667351' '23.6570841889117'\n",
            " '.459958932238193' '132.492813141684' '283' '49.3141683778234'\n",
            " '27.9856262833676' '38' '7.6570841889117' '83.6550308008214'\n",
            " '10.9199178644764' '162.328542094456' '37' '132.328542094456'\n",
            " '35.952772073922' '165' '10.9856262833676' '20.1642710472279'\n",
            " '2.59137577002053' '175' '180.985626283368' '10.3285420944559'\n",
            " '36.1642710472279' '120.657084188912' '232' '152' '8.98562628336756'\n",
            " '167' '11.0657084188912' '11.2032854209446' '5.19712525667351'\n",
            " '3.16427104722793' '60.1642710472279' '1.18275154004107'\n",
            " '21.1642710472279' '2.19712525667351' '4.19712525667351'\n",
            " '2.62833675564682' '119.952772073922' '119.958932238193'\n",
            " '9.49281314168378' '5.25667351129363' '15.3285420944559'\n",
            " '2.82135523613963' '192.985626283368' '48.6570841889117'\n",
            " '5.95687885010267' '2.29979466119097' '960' '2.36550308008214' '116'\n",
            " '19.5133470225873' '1.6570841889117'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "sentenceLength = df['ImposedSentenceAllChargeInContactEvent']\n",
        "typeOfCharge = df['SentenceTypeAllChargesAtConvictionInContactEvent']\n",
        "\n",
        "print( sentenceLength.unique()  , '\\n')\n",
        "# From printing the unique of length, we see it is in scientific notation, and some value are straight up missing.\n",
        "# So, lets convert length to numeric, and check how many of those values are null. Then, we'd wanna make the null values zero\n",
        "# in this context, because people who got no sentence should be a zero, not np.nan\n",
        "\n",
        "sentenceLength = pd.to_numeric(sentenceLength,errors='coerce')\n",
        "length_NA = sentenceLength.isnull()\n",
        "\n",
        "# We know that category 4 is the case where charges are dismissed, so we use that to change np.nans to zeros\n",
        "\n",
        "# So, we replace length to 0 when the type is 4 (for no charge), and we replace length with np.nan when the type is \n",
        "sentenceLength = sentenceLength.mask( typeOfCharge == 4, 0)\n",
        "sentenceLength = sentenceLength.mask( typeOfCharge == 9, np.nan)\n",
        "\n",
        "# Now, we replace the data in the actual dataframe with the updated sentenceLengths, and delete the temp vectors.\n",
        "df['ImposedSentenceAllChargeInContactEvent'] = sentenceLength \n",
        "del sentenceLength\n",
        "del typeOfCharge"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
